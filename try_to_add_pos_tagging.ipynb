{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries_with_lemmatization(path):\n",
    "    f = open(path)\n",
    "    queries = []\n",
    "    tags = []\n",
    "    mystem = Mystem()\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    for line in f:\n",
    "        tmp = []\n",
    "        tmp_ = []\n",
    "        text = tokenizer.tokenize(line.lower())\n",
    "        text_tagged = nltk.pos_tag(text, lang='rus')\n",
    "        \n",
    "        for i, q in enumerate(text):\n",
    "            if not np.all(np.any(np.array(list(q)).reshape(-1, 1) == np.array(list(punctuation)).reshape(1, -1), axis=1)):\n",
    "                q_ = mystem.lemmatize(q)\n",
    "                tmp.append(\"\".join(q_).split()[0])\n",
    "                tmp_.append(text_tagged[i][1])\n",
    "        queries.append(tmp)\n",
    "        tags.append(tmp_)\n",
    "    f.close()\n",
    "    return (queries, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['сибирский', 'сеть', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карта', 'россия'],\n",
       "  ['ноофен', 'для', 'какой', 'болезнь'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " [['A=pl', 'S', 'A=m', 'S', 'S'],\n",
       "  ['NUM=ciph', 'V', 'S', 'NUM=ciph'],\n",
       "  ['S', 'S', 'PR', 'S', 'S'],\n",
       "  ['V', 'PR', 'A-PRO=pl', 'A=f'],\n",
       "  ['NONLEX', 'NONLEX', 'NONLEX']],\n",
       " [['сбербанк', 'в', 'кунцево', 'плаза'],\n",
       "  ['торт', 'дикий', 'вишня'],\n",
       "  ['тася', 'кривун', 'танец', 'на', 'тнт'],\n",
       "  ['рбт', 'ру'],\n",
       "  ['toplü', 'vay', 'sexx']],\n",
       " [['V', 'PR', 'S', 'S'],\n",
       "  ['S', 'A=f', 'S'],\n",
       "  ['S', 'S', 'S', 'PR', 'S'],\n",
       "  ['V', 'S'],\n",
       "  ['NONLEX', 'NONLEX', 'NONLEX']],\n",
       " 51353,\n",
       " 21174)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/requests.uniq.train'\n",
    "train_lem = read_queries_with_lemmatization(path)\n",
    "path = 'data/requests.uniq.test'\n",
    "test_lem = read_queries_with_lemmatization(path)\n",
    "train_lem[0][:5], train_lem[1][:5], test_lem[0][:5], test_lem[1][:5], len(train_lem[0]), len(test_lem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([0.12185658, 0.16966756, 0.20326518, 0.23018925, 0.2515358 ,\n",
       "        0.27062838, 0.28792218, 0.30358516, 0.31885922, 0.32964299]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words = Counter()\n",
    "\n",
    "for d in [train_lem[0]]:\n",
    "    for q in d:\n",
    "        for word in q:\n",
    "            count_words[word] += 1\n",
    "        \n",
    "freq, counts = np.unique(np.array(list(count_words.values())), return_counts=True) \n",
    "p = counts * freq \n",
    "p = p / p.sum()\n",
    "p = np.cumsum(p)\n",
    "freq[:10], p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'A-PRO', 'A-PRO=f', 'A-PRO=m', 'A-PRO=n', 'A-PRO=pl',\n",
       "        'A-PRO=sg', 'A=brev', 'A=comp', 'A=comp2', 'A=f', 'A=m', 'A=n',\n",
       "        'A=pl', 'A=sg', 'ADV', 'ADV-PRO', 'ADV-PRO=abbr', 'ADV-PRO=comp',\n",
       "        'ADV-PRO=distort', 'ADV=abbr', 'ADV=comp', 'ADV=comp2',\n",
       "        'ANUM=ciph', 'ANUM=f', 'ANUM=m', 'ANUM=n', 'ANUM=pl', 'ANUM=sg',\n",
       "        'CONJ', 'INIT=abbr', 'INTJ', 'INTJ=distort', 'NONLEX',\n",
       "        'NONLEX=abbr', 'NUM', 'NUM=acc', 'NUM=ciph', 'NUM=comp', 'NUM=dat',\n",
       "        'NUM=f', 'NUM=gen', 'NUM=ins', 'NUM=loc', 'NUM=m', 'NUM=n',\n",
       "        'NUM=nom', 'PARENTH', 'PART', 'PR', 'PRAEDIC', 'PRAEDIC-PRO',\n",
       "        'PRAEDIC=comp', 'S', 'S-PRO', 'S-PRO=acc', 'S-PRO=dat',\n",
       "        'S-PRO=gen', 'S-PRO=ins', 'S-PRO=loc', 'S-PRO=n=sg', 'S-PRO=pl',\n",
       "        'S=m', 'V'], dtype='<U32'), 64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.unique(np.hstack(train_lem[1]))\n",
    "tmp1 = np.unique(np.hstack(test_lem[1]))\n",
    "tmp = np.hstack([tmp, tmp1])\n",
    "tmp = np.unique(tmp)\n",
    "tmp, len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tmp\n",
    "tags_to_ind = {}\n",
    "ind = 0\n",
    "for t in tags:\n",
    "    tags_to_ind[t] = ind\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сетки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Просто добавляем тег как фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_2 = KeyedVectors.load_word2vec_format(\"ft_native_300_ru_wiki_lenta_lower_case.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_tokens(emb):\n",
    "    n_tokens = 0\n",
    "    for word in count_words.keys():\n",
    "        if word in emb.vocab and count_words[word] >= 3:\n",
    "            n_tokens += 1\n",
    "    return n_tokens + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(emb, emb_size, ind_to_word, batch_x, batch_x_tags):\n",
    "    to_emb = np.zeros((len(batch_x), len(batch_x[0])+1, emb_size + len(tags)))\n",
    "    for i in range(len(batch_x)):\n",
    "        to_emb[i][0] = np.ones(emb_size + len(tags))\n",
    "        for j in range(len(batch_x[i])):\n",
    "            if batch_x[i][j] != pad_id:\n",
    "                to_emb[i][j+1][:emb_size] = emb[ind_to_word[batch_x[i][j]]]\n",
    "                if batch_x_tags[i][j] >= 0:\n",
    "                    to_emb[i][j+1][emb_size + batch_x_tags[i][j]] = 1\n",
    "    return to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb, ind_to_word, emb_size=300, lstm_units=256):\n",
    "        super(self.__class__, self).__init__()\n",
    "        n_tokens = calculate_n_tokens(emb)\n",
    "        self.lstm = nn.LSTM(emb_size + len(tags), lstm_units, batch_first=True)\n",
    "        self.logits = nn.Linear(lstm_units, n_tokens) \n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        self.ind_to_word = ind_to_word\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_tags):\n",
    "        input_emb = transform_to_features(self.emb, self.emb_size, self.ind_to_word, batch_x, batch_x_tags)\n",
    "        input_emb = torch.tensor(input_emb, dtype=torch.float32)\n",
    "        lstm_out = self.lstm(input_emb)\n",
    "        logits = self.logits(lstm_out[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = '#PAD#'\n",
    "pad_id = 0\n",
    "\n",
    "def construct_vocab(emb, count_words):\n",
    "    word_to_ind = dict()\n",
    "    word_to_ind['#PAD#'] = 0\n",
    "    ind_to_word = ['#PAD#', ]\n",
    "    \n",
    "    count = 1\n",
    "    for word in count_words.keys():\n",
    "        if count_words[word] >= 3 and word in emb.vocab:\n",
    "            ind_to_word.append(word)\n",
    "            word_to_ind[word] = count\n",
    "            count += 1\n",
    "    return ind_to_word, word_to_ind\n",
    "\n",
    "\n",
    "def as_matrix(sequences, tags, word_to_ind, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((2, len(sequences), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq[:max_len]):\n",
    "            if word in word_to_ind.keys():\n",
    "                matrix[0][i][j] = word_to_ind[word]\n",
    "                matrix[1][i][j] = tags_to_ind[tags[i][j]]\n",
    "            else:\n",
    "                matrix[0][i][j] = pad_id\n",
    "                matrix[1][i][j] = -1\n",
    "        for j in range(max_len, len(seq)):\n",
    "            matrix[0][i][j] = pad_id\n",
    "            matrix[1][i][j] = -1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(network, batch):\n",
    "    \"\"\"\n",
    "    use scalar crossentropy loss (neg llh) loss \n",
    "    \"\"\"\n",
    "    batch_x = batch[0]\n",
    "    batch_tags = batch[1]\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_tags = np.array(batch_tags)\n",
    "\n",
    "    batch_x_inp = batch_x[:, :-1]\n",
    "    batch_x_next = batch_x[:, 1:]\n",
    "    batch_tags_inp = batch_tags[:, :-1]\n",
    "    batch_tags_next = batch_tags[:, 1:]\n",
    "    \n",
    "    logits_for_next = network.forward(batch_x_inp, batch_tags_inp)\n",
    "    logits_for_next = logits_for_next[:, 1:]\n",
    "    \n",
    "    answers = torch.argmax(logits_for_next, dim=-1).numpy()\n",
    "    logits_for_next = logits_for_next.contiguous()\n",
    "    logits_for_next = logits_for_next.view(-1, logits_for_next.shape[-1])\n",
    "    \n",
    "    accr = np.array([answers == batch_x_next]) * np.array([answers != pad_id])\n",
    "    accr = accr.sum()\n",
    "    to_div = np.sum(np.array([batch_x_next != pad_id]))\n",
    "    batch_x_next = torch.tensor(batch_x_next, dtype=torch.int64)\n",
    "    batch_x_next = batch_x_next.view(-1)\n",
    "    \n",
    "    loss = F.cross_entropy(logits_for_next, batch_x_next, ignore_index=pad_id, reduction='mean')\n",
    "    \n",
    "    \n",
    "    return loss, accr, to_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(train, batch_size, word_to_ind, max_len=None):\n",
    "    random_x = np.random.randint(0, len(train[0]), size=batch_size)\n",
    "    batch_x = []\n",
    "    batch_tags = []\n",
    "    for x in random_x:\n",
    "        batch_x.append(train[0][x])\n",
    "        batch_tags.append(train[1][x])\n",
    "    return as_matrix(batch_x, batch_tags, word_to_ind, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "n_epochs = 20 \n",
    "n_batches_per_epoch = 400  \n",
    "n_validation_batches = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:03<00:00,  2.61it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, train loss: 7.415038217306137, val loss: 7.029473960399628\n",
      "\n",
      "Epoch: 0, train accr: 0.06012615392945126, val accr: 0.08210409128861676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:40<00:00,  2.90it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, train loss: 6.588179706335068, val loss: 6.555705967545509\n",
      "\n",
      "Epoch: 1, train accr: 0.10989722424536699, val accr: 0.10720432751413818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:43<00:00,  1.74it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, train loss: 6.104913908243179, val loss: 6.365627136826515\n",
      "\n",
      "Epoch: 2, train accr: 0.13640310414066315, val accr: 0.12206235872412184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:43<00:00,  1.97it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, train loss: 5.751491576433182, val loss: 6.201145070791244\n",
      "\n",
      "Epoch: 3, train accr: 0.1518869828456105, val accr: 0.12976097595574154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:45<00:00,  2.75it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, train loss: 5.463841874599456, val loss: 6.125228527188301\n",
      "\n",
      "Epoch: 4, train accr: 0.165665875974246, val accr: 0.13747347980869504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:42<00:00,  2.35it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, train loss: 5.223688576221466, val loss: 6.060811606049538\n",
      "\n",
      "Epoch: 5, train accr: 0.18190424668650257, val accr: 0.1438562857244293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:45<00:00,  2.44it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, train loss: 5.02456667304039, val loss: 6.020030668377876\n",
      "\n",
      "Epoch: 6, train accr: 0.1990015235607792, val accr: 0.1467916159954217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:43<00:00,  2.47it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, train loss: 4.8251614201068875, val loss: 5.995430633425713\n",
      "\n",
      "Epoch: 7, train accr: 0.2121998000918497, val accr: 0.15825107793179632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:43<00:00,  2.48it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8, train loss: 4.649763660430908, val loss: 5.9933482021093365\n",
      "\n",
      "Epoch: 8, train accr: 0.22680231774693438, val accr: 0.1587067299244421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:47<00:00,  2.68it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, train loss: 4.489338699579239, val loss: 6.045060223340988\n",
      "\n",
      "Epoch: 9, train accr: 0.24403225806451612, val accr: 0.15648024700021052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:42<00:00,  2.60it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10, train loss: 4.325403891801834, val loss: 6.019878289103508\n",
      "\n",
      "Epoch: 10, train accr: 0.26187012844439567, val accr: 0.16132177681473456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:47<00:00,  2.01it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11, train loss: 4.19681582570076, val loss: 6.081585231423378\n",
      "\n",
      "Epoch: 11, train accr: 0.27377579927155, val accr: 0.15985796653072704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:42<00:00,  2.76it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12, train loss: 4.03578318297863, val loss: 6.066061696410179\n",
      "\n",
      "Epoch: 12, train accr: 0.29419314742758323, val accr: 0.160538179768949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:41<00:00,  2.31it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13, train loss: 3.9093258064985275, val loss: 6.077114847302437\n",
      "\n",
      "Epoch: 13, train accr: 0.3083000203265804, val accr: 0.1633900956308436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:40<00:00,  2.30it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14, train loss: 3.779133513569832, val loss: 6.045979431271553\n",
      "\n",
      "Epoch: 14, train accr: 0.32664359861591696, val accr: 0.16685357818742552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:46<00:00,  2.33it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15, train loss: 3.6739465874433517, val loss: 6.1165186882019045\n",
      "\n",
      "Epoch: 15, train accr: 0.33969682230869, val accr: 0.16361121946030283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:47<00:00,  2.36it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16, train loss: 3.5566707360744476, val loss: 6.144429913163185\n",
      "\n",
      "Epoch: 16, train accr: 0.3544253050206465, val accr: 0.16442361894024804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:46<00:00,  2.56it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17, train loss: 3.459779422879219, val loss: 6.2179121434688565\n",
      "\n",
      "Epoch: 17, train accr: 0.3693340342080789, val accr: 0.16126997476871321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:42<00:00,  2.48it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18, train loss: 3.383422926068306, val loss: 6.212906065583229\n",
      "\n",
      "Epoch: 18, train accr: 0.38028302014056575, val accr: 0.16483516483516483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:51<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 19, train loss: 3.2591949808597565, val loss: 6.271716690063476\n",
      "\n",
      "Epoch: 19, train accr: 0.3971612212104981, val accr: 0.15462953340953964\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "ind_to_word, word_to_ind = construct_vocab(emb_2, count_words)\n",
    "network = Net(emb_2, ind_to_word)\n",
    "opt = Adam(network.parameters())\n",
    "\n",
    "train_loss, val_loss, train_accr, val_accr = [], [], [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss_=0\n",
    "    train_accr_=0\n",
    "    to_div = 0\n",
    "    network.train(True)\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        \n",
    "        loss_t, accr_t, to_div_t = compute_loss(network, generate_batch(train_lem, batch_size, word_to_ind))\n",
    "        \n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss_ += loss_t.item()\n",
    "        train_accr_ += accr_t.item()\n",
    "        to_div += to_div_t\n",
    "        \n",
    "    train_loss_ /= n_batches_per_epoch\n",
    "    #train_accr_ /= n_batches_per_epoch\n",
    "    train_accr_ /= to_div\n",
    "    \n",
    "    val_loss_=0\n",
    "    val_accr_=0\n",
    "    to_div = 0\n",
    "    network.train(False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        loss_t, accr_t, to_div_t = compute_loss(network, generate_batch(test_lem, batch_size, word_to_ind))\n",
    "        \n",
    "        val_loss_ += loss_t.item()\n",
    "        val_accr_ += accr_t.item()\n",
    "        to_div += to_div_t\n",
    "        \n",
    "    val_loss_ /= n_validation_batches\n",
    "    #val_accr_ /= n_validation_batches\n",
    "    val_accr_ /= to_div\n",
    "    \n",
    "    train_loss.append(train_loss_)\n",
    "    val_loss.append(val_loss_)\n",
    "    train_accr.append(train_accr_)\n",
    "    val_accr.append(val_accr_)\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss_, val_loss_))\n",
    "    print('\\nEpoch: {}, train accr: {}, val accr: {}'.format(epoch, train_accr_, val_accr_))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно попробовать предсказывать тэг и ввести это в лосс, может качество поменяется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(), 'add_pos_tagging_as_feature.pwf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accr(network, batch):\n",
    "    batch_x = batch[0]\n",
    "    batch_tag = batch[1]\n",
    "    \n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_tag = np.array(batch_tag)\n",
    "    batch_x_inp = batch_x[:, :-1]\n",
    "    batch_x_next = batch_x[:, 1:]\n",
    "    batch_tag_inp = batch_tag[:, :-1]\n",
    "    batch_tag_next = batch_tag[:, 1:]\n",
    "    \n",
    "    logits_for_next = network.forward(batch_x_inp, batch_tag_inp)\n",
    "    logits_for_next = logits_for_next[:, 1:]\n",
    "    \n",
    "    answers = torch.argmax(logits_for_next, dim=-1).numpy()\n",
    "    \n",
    "    accr = np.array([answers == batch_x_next]) * np.array([answers != pad_id])\n",
    "    accr = accr[0]\n",
    "    accr = accr.sum(axis=0)\n",
    "    to_divide = np.array([batch_x_next != pad_id])[0].sum(axis=0)\n",
    "    \n",
    "    return accr, to_divide\n",
    "\n",
    "def get_batch(data, left, right, batch_size, word_to_ind, max_len=None):\n",
    "    slice_x = np.arange(left, right, 1)\n",
    "    batch_x = []\n",
    "    batch_tag = []\n",
    "    for x in slice_x:\n",
    "        batch_x.append(data[0][x])\n",
    "        batch_tag.append(data[1][x])\n",
    "    return as_matrix(batch_x, batch_tag, word_to_ind, max_len)\n",
    "\n",
    "def try_lengthes(data):\n",
    "    accr = np.zeros(np.max(list(map(len, data[0]))))\n",
    "    to_div = np.zeros(np.max(list(map(len, data[0]))))\n",
    "    \n",
    "    for _ in tqdm_notebook(range(0, len(data[0])-batch_size, batch_size)):\n",
    "        accr_t, div_t = compute_accr(network, get_batch(data, _, _+batch_size, batch_size, word_to_ind))\n",
    "        accr[:len(accr_t)] += accr_t\n",
    "        to_div[:len(div_t)] += div_t\n",
    "    eps = 1\n",
    "    return accr / (to_div + eps), accr.sum() / to_div.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_pad(data):\n",
    "    to_pad = 0\n",
    "    all_ = 0\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            all_ += 1\n",
    "            if word not in word_to_ind.keys():\n",
    "                to_pad += 1\n",
    "    return to_pad / all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6564a008527b4987ab9899209c1ec92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=802), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.22534229, 0.3419334 , 0.46927353, 0.51727253, 0.55219961,\n",
       "        0.5572743 , 0.57848837, 0.58447489, 0.56936226, 0.54239257]),\n",
       " 0.4007140359841863)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "on_train, all_accr = try_lengthes(train_lem)\n",
    "on_train[:10], all_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dad748fd7a7480d939b4e603f541a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=330), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.13774834, 0.14425538, 0.16985902, 0.1910342 , 0.18946509,\n",
       "        0.19265442, 0.19807281, 0.18886861, 0.16057234, 0.11239193]),\n",
       " 0.1608981155306269)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_test, all_accr = try_lengthes(test_lem)\n",
    "on_test[:10], all_accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* попробуем учить предсказание следующего тега"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
