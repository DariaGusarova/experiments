{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(path):\n",
    "    f = open(path)\n",
    "    queries = []\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    for line in f:\n",
    "        tmp = []\n",
    "        for q in tokenizer.tokenize(line.lower()):\n",
    "            if not np.all(np.any(np.array(list(q)).reshape(-1, 1) == np.array(list(punctuation)).reshape(1, -1), axis=1)):\n",
    "                tmp.append(q)\n",
    "        queries.append(tmp)\n",
    "    f.close()\n",
    "    return queries\n",
    "\n",
    "def read_queries_with_lemmatization(path):\n",
    "    f = open(path)\n",
    "    queries = []\n",
    "    mystem = Mystem()\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    for line in f:\n",
    "        tmp = []\n",
    "        for q in tokenizer.tokenize(line.lower()):\n",
    "            if not np.all(np.any(np.array(list(q)).reshape(-1, 1) == np.array(list(punctuation)).reshape(1, -1), axis=1)):\n",
    "                q_ = mystem.lemmatize(q)\n",
    "                tmp.append(\"\".join(q_).split()[0])\n",
    "        queries.append(tmp)\n",
    "    f.close()\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['сибирские', 'сети', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карте', 'россии'],\n",
       "  ['ноофен', 'для', 'каких', 'болезней'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " [['сибирский', 'сеть', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карта', 'россия'],\n",
       "  ['ноофен', 'для', 'какой', 'болезнь'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " [['сбербанк', 'в', 'кунцево', 'плаза'],\n",
       "  ['торт', 'дикая', 'вишня'],\n",
       "  ['тася', 'кривун', 'танцы', 'на', 'тнт'],\n",
       "  ['рбт', 'ру'],\n",
       "  ['toplü', 'vay', 'sexx']],\n",
       " [['сбербанк', 'в', 'кунцево', 'плаза'],\n",
       "  ['торт', 'дикий', 'вишня'],\n",
       "  ['тася', 'кривун', 'танец', 'на', 'тнт'],\n",
       "  ['рбт', 'ру'],\n",
       "  ['toplü', 'vay', 'sexx']],\n",
       " 51353,\n",
       " 21174)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/requests.uniq.train'\n",
    "train = read_queries(path)\n",
    "train_lem = read_queries_with_lemmatization(path)\n",
    "path = 'data/requests.uniq.test'\n",
    "test = read_queries(path)\n",
    "test_lem = read_queries_with_lemmatization(path)\n",
    "train[:5], train_lem[:5], test[:5], test_lem[:5], len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3257316609654124"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "were_changed = 0\n",
    "all_ = 0\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])):\n",
    "        all_ += 1\n",
    "        if train[i][j] != train_lem[i][j]:\n",
    "            were_changed += 1\n",
    "were_changed / all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([0.17574006, 0.24519813, 0.29055625, 0.32405664, 0.35185578,\n",
       "        0.37601319, 0.39457541, 0.41182944, 0.42762059, 0.4410561 ]))"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words = Counter()\n",
    "\n",
    "for d in [train]:\n",
    "    for q in d:\n",
    "        for word in q:\n",
    "            count_words[word] += 1\n",
    "        \n",
    "freq, counts = np.unique(np.array(list(count_words.values())), return_counts=True) \n",
    "p = counts * freq \n",
    "p = p / p.sum()\n",
    "p = np.cumsum(p)\n",
    "freq[:10], p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59343, 11721)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts), np.sum(counts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([0.12185658, 0.16966756, 0.20326518, 0.23018925, 0.2515358 ,\n",
       "        0.27062838, 0.28792218, 0.30358516, 0.31885922, 0.32964299]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words = Counter()\n",
    "\n",
    "for d in [train_lem]:\n",
    "    for q in d:\n",
    "        for word in q:\n",
    "            count_words[word] += 1\n",
    "        \n",
    "freq, counts = np.unique(np.array(list(count_words.values())), return_counts=True) \n",
    "p = counts * freq \n",
    "p = p / p.sum()\n",
    "p = np.cumsum(p)\n",
    "freq[:10], p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43111, 10130)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts), np.sum(counts[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM с различными эмбеддингами\n",
    "\n",
    "### с лемматизацией и без"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = KeyedVectors.load_word2vec_format(\"wiki.ru.vec\")\n",
    "emb_1 = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
    "emb_2 = KeyedVectors.load_word2vec_format(\"ft_native_300_ru_wiki_lenta_lower_case.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_tokens(emb):\n",
    "    n_tokens = 0\n",
    "    for word in count_words.keys():\n",
    "        if word in emb.vocab and count_words[word] >= 3:\n",
    "            n_tokens += 1\n",
    "    return n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11110, 10627, 11453)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_n_tokens(emb), calculate_n_tokens(emb_1), calculate_n_tokens(emb_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(emb, emb_size, ind_to_word, batch_x):\n",
    "    to_emb = np.zeros((len(batch_x), len(batch_x[0])+1, emb_size))\n",
    "    for i in range(len(batch_x)):\n",
    "        to_emb[i][0] = np.ones(emb_size)\n",
    "        for j in range(len(batch_x[i])):\n",
    "            if batch_x[i][j] != pad_id:\n",
    "                to_emb[i][j+1] = emb[ind_to_word[batch_x[i][j]]]\n",
    "    return to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb, ind_to_word, emb_size=300, lstm_units=256):\n",
    "        super(self.__class__, self).__init__()\n",
    "        n_tokens = calculate_n_tokens(emb)\n",
    "        self.lstm = nn.LSTM(emb_size, lstm_units, batch_first=True)\n",
    "        self.logits = nn.Linear(lstm_units, n_tokens) \n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        self.ind_to_word = ind_to_word\n",
    "        \n",
    "    def forward(self, batch_x):\n",
    "        input_emb = transform_to_features(self.emb, self.emb_size, ind_to_word, batch_x)\n",
    "        input_emb = torch.tensor(input_emb, dtype=torch.float32)\n",
    "        lstm_out = self.lstm(input_emb)\n",
    "        logits = self.logits(lstm_out[0])\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = '#PAD#'\n",
    "pad_id = 0\n",
    "\n",
    "def construct_vocab(emb):\n",
    "    word_to_ind = dict()\n",
    "    word_to_ind['#PAD#'] = 0\n",
    "    ind_to_word = ['#PAD#', ]\n",
    "    \n",
    "    count = 1\n",
    "    for word in count_words.keys():\n",
    "        if count_words[word] >= 3 and word in emb.vocab:\n",
    "            ind_to_word.append(word)\n",
    "            word_to_ind[word] = count\n",
    "            count += 1\n",
    "    return ind_to_word, word_to_ind\n",
    "\n",
    "def as_matrix(sequences, word_to_ind, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq[:max_len]):\n",
    "            if word in word_to_ind.keys():\n",
    "                matrix[i][j] = word_to_ind[word]\n",
    "            else:\n",
    "                matrix[i][j] = pad_id\n",
    "        for j in range(max_len, len(seq)):\n",
    "            matrix[i][j] = pad_id\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['сибирские', 'сети', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карте', 'россии'],\n",
       "  ['ноофен', 'для', 'каких', 'болезней'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " array([[ 0,  1,  2,  3,  0],\n",
       "        [ 0,  0,  4,  0,  0],\n",
       "        [ 5,  0,  6,  7,  8],\n",
       "        [ 0,  9, 10, 11,  0],\n",
       "        [12, 13, 14,  0,  0]]))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_word, word_to_ind = construct_vocab(emb)\n",
    "train[:5], as_matrix(train[:5], word_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_word, word_to_ind = construct_vocab(emb)\n",
    "network = Net(emb, ind_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([5, 6, 11110])\n"
     ]
    }
   ],
   "source": [
    "dummy_batch_x = as_matrix(train[:5], word_to_ind)\n",
    "\n",
    "dummy_logits = network.forward(dummy_batch_x)\n",
    "\n",
    "print('shape:', dummy_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(network, batch_x):\n",
    "    \"\"\"\n",
    "    use scalar crossentropy loss (neg llh) loss \n",
    "    \"\"\"\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_x_inp = batch_x[:, :-1]\n",
    "    batch_x_next = batch_x[:, 1:]\n",
    "    \n",
    "    logits_for_next = network.forward(batch_x_inp)\n",
    "    logits_for_next = logits_for_next[:, 1:]\n",
    "    \n",
    "    answers = torch.argmax(logits_for_next, dim=-1).numpy()\n",
    "    logits_for_next = logits_for_next.contiguous()\n",
    "    logits_for_next = logits_for_next.view(-1, logits_for_next.shape[-1])\n",
    "    \n",
    "    accr = np.array([answers == batch_x_next]) * np.array([answers != pad_id])\n",
    "    accr = accr.mean()\n",
    "    batch_x_next = torch.tensor(batch_x_next, dtype=torch.int64)\n",
    "    batch_x_next = batch_x_next.view(-1)\n",
    "    \n",
    "    loss = F.cross_entropy(logits_for_next, batch_x_next, ignore_index=pad_id, reduction='mean')\n",
    "    \n",
    "    \n",
    "    return loss, accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss, dummy_accr = compute_loss(network, dummy_batch_x)\n",
    "\n",
    "assert dummy_loss.shape == torch.Size([]), 'loss must be scalar'\n",
    "assert dummy_loss.data.numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "assert all(param.grad is not None for param in network.parameters()), \\\n",
    "        'loss should depend differentiably on all neural network weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(train, batch_size, word_to_ind, max_len=None):\n",
    "    random_x = np.random.randint(0, len(train), size=batch_size)\n",
    "    batch_x = []\n",
    "    for x in random_x:\n",
    "        batch_x.append(train[x])\n",
    "    return as_matrix(batch_x, word_to_ind, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "n_epochs = 100  \n",
    "n_batches_per_epoch = 300  \n",
    "n_validation_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802.390625, 330.84375)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) / batch_size, len(test) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emb и без лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:27<00:00,  2.51it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, train loss: 7.759962277412415, val loss: 7.447972011566162\n",
      "\n",
      "Epoch: 0, train accr: 0.011002072271073414, val accr: 0.013308434519531129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:20<00:00,  2.30it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, train loss: 7.055293418566386, val loss: 6.7997199106216435\n",
      "\n",
      "Epoch: 1, train accr: 0.02167520221429876, val accr: 0.022811159839641902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:13<00:00,  2.34it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, train loss: 6.493653011322022, val loss: 6.434842958450317\n",
      "\n",
      "Epoch: 2, train accr: 0.028725509839979277, val accr: 0.027242298317545023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 61/300 [00:27<01:49,  2.19it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.7/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-688e6c94a1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-386-4f62cef7d70b>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(network, batch_x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_x_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_for_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.7/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ind_to_word, word_to_ind = construct_vocab(emb)\n",
    "network = Net(emb, ind_to_word)\n",
    "opt = Adam(network.parameters())\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss=0\n",
    "    train_accr=0\n",
    "    network.train(True)\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        \n",
    "        loss_t, accr_t = compute_loss(network, generate_batch(train, batch_size, word_to_ind))\n",
    "        \n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss += loss_t.item()\n",
    "        train_accr += accr_t.item()\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    train_accr /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss=0\n",
    "    val_accr=0\n",
    "    network.train(False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        loss_t, accr_t = compute_loss(network, generate_batch(test, batch_size, word_to_ind))\n",
    "        \n",
    "        val_loss += loss_t.item()\n",
    "        val_accr += accr_t.item()\n",
    "    val_loss /= n_validation_batches\n",
    "    val_accr /= n_validation_batches\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "    print('\\nEpoch: {}, train accr: {}, val accr: {}'.format(epoch, train_accr, val_accr))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
