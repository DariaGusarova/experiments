{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(path):\n",
    "    f = open(path)\n",
    "    queries = []\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    for line in f:\n",
    "        tmp = []\n",
    "        for q in tokenizer.tokenize(line.lower()):\n",
    "            if not np.all(np.any(np.array(list(q)).reshape(-1, 1) == np.array(list(punctuation)).reshape(1, -1), axis=1)):\n",
    "                tmp.append(q)\n",
    "        queries.append(tmp)\n",
    "    f.close()\n",
    "    return queries\n",
    "\n",
    "def read_queries_with_lemmatization(path):\n",
    "    f = open(path)\n",
    "    queries = []\n",
    "    mystem = Mystem()\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    \n",
    "    for line in f:\n",
    "        tmp = []\n",
    "        for q in tokenizer.tokenize(line.lower()):\n",
    "            if not np.all(np.any(np.array(list(q)).reshape(-1, 1) == np.array(list(punctuation)).reshape(1, -1), axis=1)):\n",
    "                q_ = mystem.lemmatize(q)\n",
    "                tmp.append(\"\".join(q_).split()[0])\n",
    "        queries.append(tmp)\n",
    "    f.close()\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['сибирские', 'сети', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карте', 'россии'],\n",
       "  ['ноофен', 'для', 'каких', 'болезней'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " [['сибирский', 'сеть', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карта', 'россия'],\n",
       "  ['ноофен', 'для', 'какой', 'болезнь'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " [['сбербанк', 'в', 'кунцево', 'плаза'],\n",
       "  ['торт', 'дикая', 'вишня'],\n",
       "  ['тася', 'кривун', 'танцы', 'на', 'тнт'],\n",
       "  ['рбт', 'ру'],\n",
       "  ['toplü', 'vay', 'sexx']],\n",
       " [['сбербанк', 'в', 'кунцево', 'плаза'],\n",
       "  ['торт', 'дикий', 'вишня'],\n",
       "  ['тася', 'кривун', 'танец', 'на', 'тнт'],\n",
       "  ['рбт', 'ру'],\n",
       "  ['toplü', 'vay', 'sexx']],\n",
       " 51353,\n",
       " 21174)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/requests.uniq.train'\n",
    "train = read_queries(path)\n",
    "train_lem = read_queries_with_lemmatization(path)\n",
    "path = 'data/requests.uniq.test'\n",
    "test = read_queries(path)\n",
    "test_lem = read_queries_with_lemmatization(path)\n",
    "train[:5], train_lem[:5], test[:5], test_lem[:5], len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3257316609654124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "were_changed = 0\n",
    "all_ = 0\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])):\n",
    "        all_ += 1\n",
    "        if train[i][j] != train_lem[i][j]:\n",
    "            were_changed += 1\n",
    "were_changed / all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([0.17574006, 0.24519813, 0.29055625, 0.32405664, 0.35185578,\n",
       "        0.37601319, 0.39457541, 0.41182944, 0.42762059, 0.4410561 ]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words = Counter()\n",
    "\n",
    "for d in [train]:\n",
    "    for q in d:\n",
    "        for word in q:\n",
    "            count_words[word] += 1\n",
    "        \n",
    "freq, counts = np.unique(np.array(list(count_words.values())), return_counts=True) \n",
    "p = counts * freq \n",
    "p = p / p.sum()\n",
    "p = np.cumsum(p)\n",
    "freq[:10], p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59343, 11721)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts), np.sum(counts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([0.12185658, 0.16966756, 0.20326518, 0.23018925, 0.2515358 ,\n",
       "        0.27062838, 0.28792218, 0.30358516, 0.31885922, 0.32964299]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words = Counter()\n",
    "\n",
    "for d in [train_lem]:\n",
    "    for q in d:\n",
    "        for word in q:\n",
    "            count_words[word] += 1\n",
    "        \n",
    "freq, counts = np.unique(np.array(list(count_words.values())), return_counts=True) \n",
    "p = counts * freq \n",
    "p = p / p.sum()\n",
    "p = np.cumsum(p)\n",
    "freq[:10], p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43111, 10130)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts), np.sum(counts[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM с различными эмбеддингами\n",
    "\n",
    "### с лемматизацией и без"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = KeyedVectors.load_word2vec_format(\"wiki.ru.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1 = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_2 = KeyedVectors.load_word2vec_format(\"ft_native_300_ru_wiki_lenta_lower_case.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_tokens(emb):\n",
    "    n_tokens = 0\n",
    "    for word in count_words.keys():\n",
    "        if word in emb.vocab and count_words[word] >= 3:\n",
    "            n_tokens += 1\n",
    "    return n_tokens + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11110, 10627, 11453)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_n_tokens(emb), calculate_n_tokens(emb_1), calculate_n_tokens(emb_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(emb, emb_size, ind_to_word, batch_x):\n",
    "    to_emb = np.zeros((len(batch_x), len(batch_x[0])+1, emb_size))\n",
    "    for i in range(len(batch_x)):\n",
    "        to_emb[i][0] = np.ones(emb_size)\n",
    "        for j in range(len(batch_x[i])):\n",
    "            if batch_x[i][j] != pad_id:\n",
    "                to_emb[i][j+1] = emb[ind_to_word[batch_x[i][j]]]\n",
    "    return to_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, emb, ind_to_word, emb_size=300, lstm_units=256):\n",
    "        super(self.__class__, self).__init__()\n",
    "        n_tokens = calculate_n_tokens(emb)\n",
    "        self.lstm = nn.LSTM(emb_size, lstm_units, batch_first=True)\n",
    "        self.logits = nn.Linear(lstm_units, n_tokens) \n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        self.ind_to_word = ind_to_word\n",
    "        \n",
    "    def forward(self, batch_x):\n",
    "        input_emb = transform_to_features(self.emb, self.emb_size, self.ind_to_word, batch_x)\n",
    "        input_emb = torch.tensor(input_emb, dtype=torch.float32)\n",
    "        lstm_out = self.lstm(input_emb)\n",
    "        logits = self.logits(lstm_out[0])\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = '#PAD#'\n",
    "pad_id = 0\n",
    "\n",
    "def construct_vocab(emb, count_words):\n",
    "    word_to_ind = dict()\n",
    "    word_to_ind['#PAD#'] = 0\n",
    "    ind_to_word = ['#PAD#', ]\n",
    "    \n",
    "    count = 1\n",
    "    for word in count_words.keys():\n",
    "        if count_words[word] >= 3 and word in emb.vocab:\n",
    "            ind_to_word.append(word)\n",
    "            word_to_ind[word] = count\n",
    "            count += 1\n",
    "    return ind_to_word, word_to_ind\n",
    "\n",
    "def as_matrix(sequences, word_to_ind, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, word in enumerate(seq[:max_len]):\n",
    "            if word in word_to_ind.keys():\n",
    "                matrix[i][j] = word_to_ind[word]\n",
    "            else:\n",
    "                matrix[i][j] = pad_id\n",
    "        for j in range(max_len, len(seq)):\n",
    "            matrix[i][j] = pad_id\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['сибирские', 'сети', 'личный', 'кабинет', 'бердск'],\n",
       "  ['1', 'сантим', 'алжир', '1964'],\n",
       "  ['река', 'колыма', 'на', 'карте', 'россии'],\n",
       "  ['ноофен', 'для', 'каких', 'болезней'],\n",
       "  ['маус', 'хаус', 'спб']],\n",
       " array([[ 0,  1,  2,  3,  0],\n",
       "        [ 0,  0,  4,  0,  0],\n",
       "        [ 5,  0,  6,  7,  8],\n",
       "        [ 0,  9, 10, 11,  0],\n",
       "        [12, 13, 14,  0,  0]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_word, word_to_ind = construct_vocab(emb, count_words)\n",
    "train[:5], as_matrix(train[:5], word_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_word, word_to_ind = construct_vocab(emb, count_words)\n",
    "network = Net(emb, ind_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([5, 6, 11111])\n"
     ]
    }
   ],
   "source": [
    "dummy_batch_x = as_matrix(train[:5], word_to_ind)\n",
    "\n",
    "dummy_logits = network.forward(dummy_batch_x)\n",
    "\n",
    "print('shape:', dummy_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(network, batch_x):\n",
    "    \"\"\"\n",
    "    use scalar crossentropy loss (neg llh) loss \n",
    "    \"\"\"\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_x_inp = batch_x[:, :-1]\n",
    "    batch_x_next = batch_x[:, 1:]\n",
    "    \n",
    "    logits_for_next = network.forward(batch_x_inp)\n",
    "    logits_for_next = logits_for_next[:, 1:]\n",
    "    \n",
    "    answers = torch.argmax(logits_for_next, dim=-1).numpy()\n",
    "    logits_for_next = logits_for_next.contiguous()\n",
    "    logits_for_next = logits_for_next.view(-1, logits_for_next.shape[-1])\n",
    "    \n",
    "    accr = np.array([answers == batch_x_next]) * np.array([answers != pad_id])\n",
    "    accr = accr.mean()\n",
    "    batch_x_next = torch.tensor(batch_x_next, dtype=torch.int64)\n",
    "    batch_x_next = batch_x_next.view(-1)\n",
    "    \n",
    "    loss = F.cross_entropy(logits_for_next, batch_x_next, ignore_index=pad_id, reduction='mean')\n",
    "    \n",
    "    \n",
    "    return loss, accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss, dummy_accr = compute_loss(network, dummy_batch_x)\n",
    "\n",
    "assert dummy_loss.shape == torch.Size([]), 'loss must be scalar'\n",
    "assert dummy_loss.data.numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "assert all(param.grad is not None for param in network.parameters()), \\\n",
    "        'loss should depend differentiably on all neural network weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(train, batch_size, word_to_ind, max_len=None):\n",
    "    random_x = np.random.randint(0, len(train), size=batch_size)\n",
    "    batch_x = []\n",
    "    for x in random_x:\n",
    "        batch_x.append(train[x])\n",
    "    return as_matrix(batch_x, word_to_ind, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "n_epochs = 100  \n",
    "n_batches_per_epoch = 400  \n",
    "n_validation_batches = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802.390625, 330.84375)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) / batch_size, len(test) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emb и без лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:09<00:00,  2.11it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, train loss: 7.622671223878861, val loss: 7.1394746661186215\n",
      "\n",
      "Epoch: 0, train accr: 0.01339046165771652, val accr: 0.018097800048641612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:15<00:00,  1.96it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, train loss: 6.71602601647377, val loss: 6.575458806753159\n",
      "\n",
      "Epoch: 1, train accr: 0.026087059246560535, val accr: 0.025448395451815924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:08<00:00,  1.55it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, train loss: 6.160353838205338, val loss: 6.264864519238472\n",
      "\n",
      "Epoch: 2, train accr: 0.03143006894256096, val accr: 0.02903436289460306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:03<00:00,  2.42it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, train loss: 5.731523013114929, val loss: 6.107632464170456\n",
      "\n",
      "Epoch: 3, train accr: 0.03726837527910878, val accr: 0.03154029908142869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:03<00:00,  2.32it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, train loss: 5.418812232017517, val loss: 6.057292622327805\n",
      "\n",
      "Epoch: 4, train accr: 0.04167745368437054, val accr: 0.033277360663551046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:58<00:00,  2.07it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, train loss: 5.114300377368927, val loss: 5.950650975108147\n",
      "\n",
      "Epoch: 5, train accr: 0.04387270713642712, val accr: 0.0360907957266926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:57<00:00,  2.11it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, train loss: 4.897996553182602, val loss: 5.935343831777573\n",
      "\n",
      "Epoch: 6, train accr: 0.04868802166212914, val accr: 0.03443580243595918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:56<00:00,  2.10it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, train loss: 4.682642257809639, val loss: 5.898927417397499\n",
      "\n",
      "Epoch: 7, train accr: 0.05201731820089961, val accr: 0.03742238480804301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:58<00:00,  2.17it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8, train loss: 4.481962509155274, val loss: 5.922384345531464\n",
      "\n",
      "Epoch: 8, train accr: 0.05687295924233221, val accr: 0.03637323670610014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:03<00:00,  2.39it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, train loss: 4.2960645943880085, val loss: 5.947059541940689\n",
      "\n",
      "Epoch: 9, train accr: 0.05878392365857763, val accr: 0.0351608558363137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:06<00:00,  2.31it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10, train loss: 4.114418806433678, val loss: 5.938294425606728\n",
      "\n",
      "Epoch: 10, train accr: 0.06544354355130344, val accr: 0.03687505863359641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:02<00:00,  2.31it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11, train loss: 3.9051584994792936, val loss: 5.996386176347732\n",
      "\n",
      "Epoch: 11, train accr: 0.0710163598793967, val accr: 0.037152650490249024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:00<00:00,  2.44it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12, train loss: 3.780373965501785, val loss: 5.971921709179878\n",
      "\n",
      "Epoch: 12, train accr: 0.07353332700154942, val accr: 0.03703464060149644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:06<00:00,  2.49it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13, train loss: 3.646620386838913, val loss: 6.007755082845688\n",
      "\n",
      "Epoch: 13, train accr: 0.07838597734581547, val accr: 0.036371846066940085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:56<00:00,  2.13it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14, train loss: 3.530631050467491, val loss: 6.024186983704567\n",
      "\n",
      "Epoch: 14, train accr: 0.08195720765325726, val accr: 0.03820906911922527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:56<00:00,  2.40it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15, train loss: 3.4070708465576174, val loss: 6.0635532438755035\n",
      "\n",
      "Epoch: 15, train accr: 0.08526162803328177, val accr: 0.03573004715734735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:01<00:00,  1.63it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16, train loss: 3.3217184180021286, val loss: 6.086744508147239\n",
      "\n",
      "Epoch: 16, train accr: 0.087338901427904, val accr: 0.03626057822515387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:00<00:00,  2.23it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17, train loss: 3.1840603721141814, val loss: 6.183137547969818\n",
      "\n",
      "Epoch: 17, train accr: 0.09215102069543843, val accr: 0.03503935518796831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:12<00:00,  2.03it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18, train loss: 3.1010964900255202, val loss: 6.121600332856178\n",
      "\n",
      "Epoch: 18, train accr: 0.09503738744379858, val accr: 0.03606126724890567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:03<00:00,  2.49it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 19, train loss: 3.017573115229607, val loss: 6.150854456424713\n",
      "\n",
      "Epoch: 19, train accr: 0.1000758219572616, val accr: 0.037481509818435045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [03:04<00:00,  2.07it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20, train loss: 2.9091727930307387, val loss: 6.201415035128593\n",
      "\n",
      "Epoch: 20, train accr: 0.1013911625769927, val accr: 0.03894795439755198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 103/400 [00:48<02:20,  2.12it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c964b1c29e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4f62cef7d70b>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(network, batch_x)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_x_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlogits_for_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlogits_for_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_for_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-70e7f23ffa76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ind_to_word, word_to_ind = construct_vocab(emb, count_words)\n",
    "network = Net(emb, ind_to_word)\n",
    "opt = Adam(network.parameters())\n",
    "\n",
    "train_loss, val_loss, train_accr, val_accr = [], [], [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss_=0\n",
    "    train_accr_=0\n",
    "    network.train(True)\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        \n",
    "        loss_t, accr_t = compute_loss(network, generate_batch(train, batch_size, word_to_ind))\n",
    "        \n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_loss_ += loss_t.item()\n",
    "        train_accr_ += accr_t.item()\n",
    "        \n",
    "    train_loss_ /= n_batches_per_epoch\n",
    "    train_accr_ /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss_=0\n",
    "    val_accr_=0\n",
    "    network.train(False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        loss_t, accr_t = compute_loss(network, generate_batch(test, batch_size, word_to_ind))\n",
    "        \n",
    "        val_loss_ += loss_t.item()\n",
    "        val_accr_ += accr_t.item()\n",
    "    val_loss_ /= n_validation_batches\n",
    "    val_accr_ /= n_validation_batches\n",
    "    \n",
    "    train_loss.append(train_loss_)\n",
    "    val_loss.append(val_loss_)\n",
    "    train_accr.append(train_accr_)\n",
    "    val_accr.append(val_accr_)\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss_, val_loss_))\n",
    "    print('\\nEpoch: {}, train accr: {}, val accr: {}'.format(epoch, train_accr_, val_accr_))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(), 'emb_no_lemmatization.pwf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accr(network, batch_x):\n",
    "    \"\"\"\n",
    "    use scalar crossentropy loss (neg llh) loss \n",
    "    \"\"\"\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_x_inp = batch_x[:, :-1]\n",
    "    batch_x_next = batch_x[:, 1:]\n",
    "    \n",
    "    logits_for_next = network.forward(batch_x_inp)\n",
    "    logits_for_next = logits_for_next[:, 1:]\n",
    "    \n",
    "    answers = torch.argmax(logits_for_next, dim=-1).numpy()\n",
    "    accr = np.array([answers == batch_x_next]) * np.array([answers != pad_id])\n",
    "    accr = accr[0]\n",
    "    accr = accr.sum(axis=0)\n",
    "    to_divide = np.array([batch_x_next != pad_id])[0].sum(axis=0)\n",
    "    \n",
    "    return accr, to_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5,  5, 10,  3,  3,  2,  0,  0,  0]),\n",
       " array([34, 32, 30,  9, 12,  6,  2,  1,  0]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accr, to_divide = compute_accr(network, generate_batch(test, batch_size, word_to_ind))\n",
    "accr, to_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, left, right, batch_size, word_to_ind, max_len=None):\n",
    "    slice_x = np.arange(left, right, 1)\n",
    "    batch_x = []\n",
    "    for x in slice_x:\n",
    "        batch_x.append(data[x])\n",
    "    return as_matrix(batch_x, word_to_ind, max_len)\n",
    "\n",
    "def try_lengthes(data):\n",
    "    accr = np.zeros(np.max(list(map(len, data))))\n",
    "    to_div = np.zeros(np.max(list(map(len, data))))\n",
    "    \n",
    "    for _ in tqdm_notebook(range(0, len(data)-batch_size, batch_size)):\n",
    "        accr_t, div_t = compute_accr(network, get_batch(data, _, _+batch_size, batch_size, word_to_ind))\n",
    "        accr[:len(accr_t)] += accr_t\n",
    "        to_div[:len(div_t)] += div_t\n",
    "    eps = 1\n",
    "    return accr / (to_div + eps), accr.sum() / to_div.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d29daed544b42b0a5004c08efabfaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=802), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.238299  , 0.39762369, 0.55320703, 0.61953927, 0.6672619 ,\n",
       "       0.66792206, 0.68522664, 0.70788108, 0.70760234, 0.67776097,\n",
       "       0.62015504, 0.63247863, 0.67142857, 0.66292135, 0.55737705,\n",
       "       0.48214286, 0.53191489, 0.59375   , 0.42857143, 0.65384615,\n",
       "       0.5625    , 0.5       , 0.4       , 0.54545455, 0.63636364,\n",
       "       0.4       , 0.54545455, 0.5       , 0.75      , 0.2       ,\n",
       "       0.57142857, 0.33333333, 0.5       , 0.33333333, 0.6       ,\n",
       "       0.2       , 0.25      , 0.        , 0.5       , 0.4       ,\n",
       "       0.6       , 0.75      , 0.5       , 0.5       , 0.66666667,\n",
       "       0.5       , 0.66666667, 0.5       , 0.        , 0.66666667,\n",
       "       0.33333333, 0.5       , 0.66666667, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.5       , 0.5       ,\n",
       "       0.        , 0.5       , 0.5       , 0.5       , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "on_train, all_accr = try_lengthes(train)\n",
    "on_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.238299  , 0.39762369, 0.55320703, 0.61953927, 0.6672619 ,\n",
       "        0.66792206, 0.68522664, 0.70788108, 0.70760234, 0.67776097]),\n",
       " 0.4645881199890744)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_train[:10], all_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b1499ba4794b6c82fb22df8de32efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=330), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.14579439, 0.15652655, 0.18843735, 0.20594262, 0.20559548,\n",
       "       0.21215933, 0.20164609, 0.19335706, 0.15778689, 0.104     ,\n",
       "       0.12878788, 0.06024096, 0.02040816, 0.05555556, 0.12      ,\n",
       "       0.05882353, 0.06666667, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5       , 0.33333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_test, all_accr = try_lengthes(test)\n",
    "on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14579439, 0.15652655, 0.18843735, 0.20594262, 0.20559548,\n",
       "        0.21215933, 0.20164609, 0.19335706, 0.15778689, 0.104     ]),\n",
       " 0.1729730865390676)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_test[:10], all_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('emb_no_lemmatization.txt', 'w')\n",
    "for x in [train_loss, val_loss, train_accr, val_accr, on_train, on_test]:\n",
    "    print(len(x), file=f)\n",
    "    for y in x:\n",
    "        print(y, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3014991205041853, 0.34950137458897096)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def approximate_pad(data):\n",
    "    to_pad = 0\n",
    "    all_ = 0\n",
    "    for x in data:\n",
    "        for word in x:\n",
    "            all_ += 1\n",
    "            if word not in word_to_ind.keys():\n",
    "                to_pad += 1\n",
    "    return to_pad / all_\n",
    "\n",
    "pad_train = approximate_pad(train)\n",
    "pad_test = approximate_pad(test)\n",
    "pad_train, pad_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16645206, 0.2777405 , 0.3864156 , 0.43274872, 0.46608303,\n",
       "        0.46654414, 0.47863141, 0.49445555, 0.49426086, 0.47341663]),\n",
       " array([0.09483905, 0.1018203 , 0.12257824, 0.13396539, 0.13373958,\n",
       "        0.13800935, 0.1311705 , 0.1257785 , 0.10264015, 0.06765186]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_train[:10] * (1 - pad_train), on_test[:10] * (1 - pad_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to try\n",
    "\n",
    "добавить метрику в топ-5/10\n",
    "\n",
    "попробовать для других эмбеддингов и с лемматизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = TheModelClass(*args, **kwargs)\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
